\documentclass{article}
% =======PACKAGES=======
% FORMATTING
\usepackage[margin=0.625in]{geometry}
\usepackage{parskip, setspace}
\setstretch{1.15}
\renewcommand{\arraystretch}{1.25}
% TYPESETTING - MATH
\usepackage{amsmath, amsfonts}
\usepackage[ruled, linesnumbered, noend]{algorithm2e}
% RICH
\usepackage{graphicx, caption}
\usepackage{hyperref}
% BIBLIOGRAPHY
\usepackage[
backend=biber,
sorting=ynt
]{biblatex}
\addbibresource{bib.bib}

% =======TITLE=======
\title{\vspace*{-0.625in}CS 565: Scientific Computing \\ Project 1: Random Search \& Empirical Gradient Descent\vspace*{-0.25in}}
\author{Nathan Chapman and Andrew Struthers}
\date{\today}

\begin{document}

    \maketitle

    \section{Introduction}

        Optimization algorithms play a pivotal role in various fields, ranging from machine learning and artificial intelligence to engineering and finance. Mathematical optimization is often full of high-dimension, intractable problems. These problems have solution spaces that are very difficult, if not impossible, to visualize and work with using standard approaches.  Mathematical optimization problems also typically suffer from complex geometric solution spaces, full of plateaus, holes, and other features that make effective traversal difficult. Efficient and reliable algorithms have been created that navigate complex landscapes of these high-dimensional benchmark problem. 

        Classic benchmark problems, such as Ackley's, De Jong's, and Schwefel's functions, test various strengths and weaknesses of these algorithms. The classic benchmark functions contain many complex geometric structures in their solution spaces, making them effective at rigorously testing new optimization algorithms. With the ever-increasing complexity of real-world optimization challenges, the development of consistent, effective, and fast optimization techniques has become a popular area of study among Computer Scientists. 

        This paper discusses and analyzes the intersection of pseudo-random number generators (PRNGs) and their role in generating solution vectors for these multi-dimensional benchmark problems. In addition to investigating the impact of various pseudo-random number generators (PRNGs) on initial solution generation, we also use empirical gradient descent to improve upon the initial solution population. By using statistically strong PRNGs to generate initial solutions, then applying a heuristic-based empirical gradient descent, we can further explore the solution space with the aim of seeking the global optimum.

        Optimization algorithms, such as empirical gradient descent, have proven successful in mathematical optimization problems; however, their performance relies heavily on the initial solution vectors. By combining strong PRNGs with empirical gradient descent, we hope to traverse complex fitness landscapes posed by benchmark problems effectively and reliably.

        The benchmark problems used in this paper each have distinct characteristics that rigorously test optimization algorithms. Some of the functions, like Ackley’s, Egg Holder, and Rastrigin, are known for their non-convexity and multiple local optima. Functions like Rosenbrock’s Saddle contain valleys and steep ridges, while functions like Stretch V Sine Wave pose varying solution spaces and are computationally expensive to compute.

        We use a diverse set of generators, where each generator has unique properties that might impact optimization outcomes. Because we are combining a random based initial solution approach with empirical gradient descent, the choice of PRNG will influence not only the diversity of the initial solution pool the trajectories followed during gradient descent iterations. The Mersenne Twister is a classic generator used very frequently in many applications due to its long period and statistical properties. We will also be using Xoshiro256++, which is hardware based and is capable of fast execution without sacrificing excellent statistical performance. We also consider the Lagged Fibonacci Generator (LFib), a simple but powerful recurrence based PRNG. Additionally, we look at using CUDA for massively parallel experimentation with a larger number of experiments for each algorithm. CUDA has native implementations of the XORWow, Mersenne Twister, and other parallelizable PRNG algorithms.

        Through this study, we seek to devise a strategy of combining PRNGs and empirical gradient descent to navigate complex search spaces efficiently. This combined approach will improve the optimization algorithm convergence as well as the ability to find the global optima. This combined approach aims utilizes the best parts of both randomness and deterministic refinement in solving high-dimensional optimization problems.

    \section{Methods}

        \subsection{Pseudo-random Number Generation}

            \subsubsection{Xoshiro256++}

            \subsubsection{Mersenne Twister}
            The Mersenne Twister is a very commonly used general purpose PRNG that introduces a long period, which was uncommon in other PRNGs around the time, which often suffered from having a short period. The Mersenne Twister algorithm has a period of $2^19937-1$. This makes the Mersenne Twister a very effective as a PRNG. Mersenne Twister is one of two PRNGs used in the IBM Statistical Package for the Social Sciences (SPSS), and is implemented in many different programming languages as a native or STL PRNG. While a long period doesn't necessarily equate to a high quality PRNG, the Mersenne Twister is also $k$-distributed to a 32-bit accuracy for every $1\leq k\leq623$. This algorithm is very fast, but it does require a state buffer to operate. The state buffer for the MT algorithm is quite large compared to other variants, so while it is quite fast and high quality, it does require more space to operate. On machines of sufficient size, this typically isn't a concern, but on smaller machines where memory is more limited, this could be unacceptable. One other drawback is that the MT can take a long time to generate output that passes various statistical randomness tests if the seed value isn't sufficiently random itself, so using operations like offsetting the start of the sequence can be beneficial. The Mersenne Twister is a very good standard PRNG, and as such, is implemented in many programming languages and software packages. We used the standard Mersenne Twister algorithm in both the Julia implementation and the CUDA implementation. 

            \subsubsection{XORWow}

The XORWow algorithm is one of the standard PRNGs implemented in the CUDA library. It is part of the XORShift family of PRNGs, and uses hardware level operations to achieve fast and efficient random number generation. CUDA uses XORWow as a standard implementation because it can be implemented to generate numbers in parallel, making full use of the GPU acceleration. XORWow repeatedly applies a bitwise XOR operation and a shift operation to its internal state, very similar to the basic XORShift, or more complex Xoshiro256, to produce a sequence of random numbers.  Because of the parallel nature of this algorithm, different GPU threads can generate streams of random numbers independently, which allows an efficient utilization of the GPU when generating many random numbers. To ensure independence when generating numbers in parallel, each thread is initialized with a unique seed that relies on some combination of the block and thread IDs. This independence allows XORWow  to produce random numbers with good statistical properties very quickly.

            \subsubsection{Philox 32}

        \subsection{Benchmarks}

        \subsection{Empirical Gradient Descent}

            Gradient descent(eq. \ref{eq:gradient}, \ref{eq:descent}, \ref{eq:while_gradient})
            
            \begin{align}
                \label{eq:gradient} \nabla f(x) &= \left\{ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots\right\} \\
                \label{eq:descent} x &\leftarrow x - \alpha \nabla f(x) \\
                \label{eq:while_gradient} \text{\texttt{while} } \Omega &< ||\nabla f(x)||
            \end{align}
            
            is a classical method in mathematical optimization because of its roots in calculus and intuitivity.  Gradient decsent has drawbacks when it comes to differentiability,  convexity, and curvature.  For example, the descent (\ref{eq:descent}) is updated as long as the size of the gradient is bigger than (eq \ref{eq:while_gradient}) zero ($\Omega = 0$) (or threshold when being done computationally $0 < \Omega \ll 1$).  As a first step to approximate the useful features of the gradient and mitigate these problems, the \emph{empirical} gradient varition can be used.  The key difference between these methods, is that instead of focusing on the size of the gradient vector (i.e. calculating its norm at every step until its close enough to zero) (eq. \ref{eq:while_gradient}), we focus solely on the change of the image of the function as (eq. \ref{eq:empirical_while})

            \begin{align}
                \label{eq:empirical_gradient} \nabla_e f(x) &= \left\{f\left(x + \delta \hat{k}_i\right) - f\left(x\right)\right\}_i \\
                \label{empirical_descent} x_{n} &\leftarrow x_{n-1} - \delta \nabla_e f(x_{n-1}) \\
                \label{eq:empirical_while} \text{\texttt{while} } f(x_{n}) &< f(x_{n-1})
            \end{align}

            where $\hat{k}_i$ is the unit vector in the $i$-th dimension.

            Because of this change in perspective, no vector norms need to be calculated, possibly leading to significant decreases in runtime.

    \section{Results}

        \begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c|c|c|c|}
                \hline
                            & Average & Standard Deviation & Minimum & Maximum & Median & Time [s] \\
                \hline
                \hline
                Ackley's One & 587.19 & 60.62 & 424.5 & 705.34 & 588.17 & 0.438 \\
                \hline
                Ackley's Two & 652.36 & 14.56 & 602.65 & 677.75 & 653.94 & 0.053 \\
                \hline
                1st De Jong's & 104726.64 & 14731.69 & 79401.66 & 136052.36 & 105329.46 & 0.022 \\
                \hline
                Egg Holder & -67.51 & 1564.88 & -3543.09 & 3168.68 & -199.16 & 0.119 \\
                \hline
                Griewangk & 635.73 & 129.05 & 354.25 & 841.71 & 663.16 & 0.065 \\
                \hline
                Rastrigin & $2.6 \times 10^{6}$ & 511998.81 & $1.6 \times 10^6$ & $2.9 \times 10^6$ & $2.5 \times 10^6$ & 0.025 \\
                \hline
                Rosenbrock & $5.5 \times 10^{10}$ & $1.5 \times 10^{10}$ & $2.6 \times 10^{10}$ & $8.2 \times 10^{10}$ & $5.6 \times 10^{10}$ & 0.055 \\
                \hline
                Schwefel's & 12265.91 & 898.52 & 10669.58 & 15107.91 & 12154.59 & 0.044 \\
                \hline
                SESW & -20.89 & 1.31 & -23.97 & -18.68 & -20.59 & 0.083 \\
                \hline
                SVSW & 94.07 & 7.9 & 77.61 & 122.17 & 93.86 & 0.081 \\
                \hline
            \end{tabular}
            \caption{Julia with Xoshiro256++}
        \end{centering}
        \end{table}

	\begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c|c|c|c|}
                \hline
                            & Average & Standard Deviation & Minimum & Maximum & Median & Time [s] \\
                \hline
                \hline
                Ackley One & 583.12 & 58.6578 & 304.542 & 842.013 & 583.285 & 0.00845 \\
                \hline
                Ackley Two & 583.037 & 16.5308 & 462.064 & 627.577 & 585.359 & 0.03711 \\
                \hline
                De Jong 1 & 100140 & 16334.8 & 29912.4 & 176241 & 99688.7 & 0.00042163\\
                \hline
                Egg Holder & -106.642 & 1565.33 & -8759.94 & 7308.14 & -99.2367 & 0.00992 \\
                \hline
                Griewangk & 625.04 & 102.136 & 199.357 & 1125.12 & 622.969 & 0.00793 \\
                \hline
                Rastrigin & 2.69884e+06 & 441123 & 902980 & 5.22212e+06 & 2.69134e+06 & 0.00374 \\
                \hline
                Rosenbrock's & 5.79781e+10 & 1.43681e+10 & 9.46702e+09 & 1.36466e+11 & 5.73733e+10 & 0.02876\\
                \hline
                Schwefel & 12556.5 & 1069.19 & 6795.11 & 17630.6 & 12568.6 & 0.00472 \\
                \hline
                SESW & -21.1838 & 1.25719 & -28.2668 & -16.7156 & -21.1149 & 0.03290 \\
                \hline
                SVSW & 303499 & 11836.1 & 225942 & 336821 & 303329 & 0.04492 \\
                \hline
            \end{tabular}
            \caption{1000000 Experiments with CUDA using XORWow}
        \end{centering}
        \end{table}

	\begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c|c|c|c|}
                \hline
                            & Average & Standard Deviation & Minimum & Maximum & Median & Time [s] \\
                \hline
                \hline
                Ackley One & 582.921 & 58.6353 & 308.961 & 843.791 & 583.149 & 0.00845 \\
                \hline
                Ackley Two & 583.059 & 16.5201 & 459.279 & 628.006 & 585.38 & 0.03711 \\
                \hline
                De Jong 1 & 100127 & 16334.8 & 30795.6 & 175801 & 99677.5 & 0.00042323\\
                \hline
                Egg Holder & -107.924 & 1563.63 & -7848.96 & 7437.9 & -102.868 & 0.00992 \\
                \hline
                Griewangk & 625.142 & 101.975 & 199.423 & 1121.05 & 623.185 & 0.00793 \\
                \hline
                Rastrigin & 2.69884e+06 & 441123 & 902980 & 5.22212e+06 & 2.69134e+06 & 0.00374 \\
                \hline
                Rosenbrock's & 5.79839e+10 & 1.43691e+10 & 9.19142e+09 & 1.34815e+11 & 5.73814e+10 & 0.02876 \\
                \hline
                Schwefel & 12559.7 & 1067.37 & 7388.06 & 17928.6 & 12570.3 & 0.00472\\
                \hline
                SESW & -21.1796 & 1.25551 & -28.1444 & -16.5571 & -21.1133 & 0.03290 \\
                \hline
                SVSW & 303499 & 11836.1 & 225942 & 336821 & 303329 & 0.04492 \\
                \hline
            \end{tabular}
            \caption{1000000 Experiments with CUDA using Mersenne Twister}
        \end{centering}
        \end{table}

	\begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c|c|c|c|}
                \hline
                            & Average & Standard Deviation & Minimum & Maximum & Median & Time [s] \\
                \hline
                \hline
                Ackley One & 583.075 & 58.607 & 304.909 & 829.551 & 583.199 & 0.00845 \\
                \hline
                Ackley Two & 583.097 & 16.5323 & 471.824 & 628.922 & 585.396 & 0.03711 \\
                \hline
                De Jong 1 & 100149 & 16344.2 & 32357.6 & 182461 & 99709 & 0.00042230\\
                \hline
                Egg Holder & -106.53 & 1565.91 & -8484.27 & 7582.61 & -99.0725 & 0.00992 \\
                \hline
                Griewangk & 625.134 & 102.023 & 205.271 & 1110.3 & 623.039 & 0.00793 \\
                \hline
                Rastrigin & 2.69884e+06 & 441123 & 902980 & 5.22212e+06 & 2.69134e+06 & 0.00374 \\
                \hline
                Rosenbrock's & 5.80077e+10 & 1.43466e+10 & 8.38982e+09 & 1.37005e+11 & 5.74165e+10 & 0.02876 \\
                \hline
                Schwefel & 12559.4 & 1068.21 & 7481.93 & 17714.3 & 12569.9 & 0.00472 \\
                \hline
                SESW & -21.1838 & 1.25621 & -27.9692 & -16.6082 & -21.115 & 0.03290\\
                \hline
                SVSW & 303516 & 11805.7 & 219669 & 336788 & 303327 & 0.04493\\
                \hline
            \end{tabular}
            \caption{1000000 Experiments with CUDA using Philox}
        \end{centering}
        \end{table}

	\begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
                \hline
                            & Average & Minimum & Time [s] & Average & Minimum & Time [s] & Average & Minimum & Time [s] \\
                \hline
                \hline
                Ackley One & 585.976 & 484.611 & 0.00067 & 582.114 & 335.513 & 0.00065 & 583.12 & 304.542 & 0.00845 \\ 
                \hline
                Ackley Two & 582.764 & 549.495 & 0.00288 & 583.354 & 468.481 & 0.00286 & 583.037 & 462.064 & 0.03711 \\
                \hline
                De Jong 1 & 94620.3 & 64505.6 & 0.00003 & 99822.7 & 39754.1 & 0.00003 & 100140 & 29912.4 & 0.00042\\
                \hline
                Egg Holder & 94.5769 & -4812.16 & 0.00076 & -103.618 & -6552.22 & 0.00076 & -106.642 & -8759.94 & 0.00992 \\
                \hline
                Griewangk & 662.552 & 504.55 & 0.0009 & 625.738 & 219.673 & 0.00065 & 625.04 & 199.357 & 0.00793 \\
                \hline
                Rastrigin & 2.63e+06 & 1.79e+06 & 0.00029 & 2.7e+06 & 894380 & 0.00029 & 2.69e+06 & 902980 & 0.00374 \\
                \hline
                Rosenbrock's & 5.98e+10 & 3.18e+10 & 0.00127 & 5.79e+10 & 1e+10 & 0.00222 & 5.79e+10 & 9.46e+09 & 0.02876\\
                \hline
                Schwefel & 12367.4 & 9750.19 & 0.00039 & 12563.7 & 8469.67 & 0.00036 & 12556.5 & 6795.11 & 0.00472 \\
                \hline
                SESW & -21.3494 & -23.4479 & 0.00253 & -21.1728 & -28.3343 & 0.00253 & -21.1838 & -28.2668 & 0.0329 \\
                \hline
                SVSW & 301685 & 269094 & 0.0035 & 302376 & 227596 & 0.00347 & 303499 & 225942 & 0.04492 \\
                \hline
            \end{tabular}
            \caption{Comparing 30, 50000, and 1000000 Tests in CUDA using XORWow}
        \end{centering}
        \end{table}
\pagebreak
    \section{Discussion}
        Compare empirical gradient to other standard methods in numerical optimization of geometric structures:
        \begin{itemize}
            \item Barzilai-Borwein method
            \item Wolfe conditions and Line search
            \item Conjugate gradient method
            \item Proximal gradient method
            \item Stochastic gradient descent
            \item Mirror descent
            \item Broyden–Fletcher–Goldfarb–Shanno algorithm
            \item Davidon–Fletcher–Powell formula
            \item Nelder–Mead method
            \item Gauss–Newton algorithm
            \item Quantum annealing
            \item Hill climbing
        \end{itemize}

    \section{Conclusion}

\end{document}
