\documentclass{article}
% =======PACKAGES=======
% FORMATTING
\usepackage[margin=0.625in]{geometry}
\usepackage{parskip, setspace}
\setstretch{1.15}
\renewcommand{\arraystretch}{1.25}
% TYPESETTING - MATH
\usepackage{amsmath, amsfonts}
\usepackage[ruled, linesnumbered, noend]{algorithm2e}
% RICH
\usepackage{graphicx, caption}
\usepackage{hyperref}
% BIBLIOGRAPHY
\usepackage[
backend=biber,
sorting=ynt
]{biblatex}
\addbibresource{bib.bib}

% =======TITLE=======
\title{\vspace*{-0.625in}CS 565: Scientific Computing \\ Project 1: Random Search \& Empirical Gradient Descent\vspace*{-0.25in}}
\author{Nathan Chapman and Andrew Struthers}
\date{\today}

\begin{document}

    \maketitle

    \section{Introduction}

        Optimization algorithms play a pivotal role in various fields, ranging from machine learning and artificial intelligence to engineering and finance. Mathematical optimization is often full of high-dimension, intractable problems. These problems have solution spaces that are very difficult, if not impossible, to visualize and work with using standard approaches.  Mathematical optimization problems also typically suffer from complex geometric solution spaces, full of plateaus, holes, and other features that make effective traversal difficult. Efficient and reliable algorithms have been created that navigate complex landscapes of these high-dimensional benchmark problem. 

        Classic benchmark problems, such as Ackley's, De Jong's, and Schwefel's functions, test various strengths and weaknesses of these algorithms. The classic benchmark functions contain many complex geometric structures in their solution spaces, making them effective at rigorously testing new optimization algorithms. With the ever-increasing complexity of real-world optimization challenges, the development of consistent, effective, and fast optimization techniques has become a popular area of study among Computer Scientists. 

        This paper discusses and analyzes the intersection of pseudo-random number generators (PRNGs) and their role in generating solution vectors for these multi-dimensional benchmark problems. In addition to investigating the impact of various pseudo-random number generators (PRNGs) on initial solution generation, we also use empirical gradient descent to improve upon the initial solution population. By using statistically strong PRNGs to generate initial solutions, then applying a heuristic-based empirical gradient descent, we can further explore the solution space with the aim of seeking the global optimum.

        Optimization algorithms, such as empirical gradient descent, have proven successful in mathematical optimization problems; however, their performance relies heavily on the initial solution vectors. By combining strong PRNGs with empirical gradient descent, we hope to traverse complex fitness landscapes posed by benchmark problems effectively and reliably.

        The benchmark problems used in this paper each have distinct characteristics that rigorously test optimization algorithms. Some of the functions, like Ackley’s, Egg Holder, and Rastrigin, are known for their non-convexity and multiple local optima. Functions like Rosenbrock’s Saddle contain valleys and steep ridges, while functions like Stretch V Sine Wave pose varying solution spaces and are computationally expensive to compute.

        We use a diverse set of generators, where each generator has unique properties that might impact optimization outcomes. Because we are combining a random based initial solution approach with empirical gradient descent, the choice of PRNG will influence not only the diversity of the initial solution pool the trajectories followed during gradient descent iterations. The Mersenne Twister is a classic generator used very frequently in many applications due to its long period and statistical properties. We will also be using Xoshiro256++, which is hardware based and is capable of fast execution without sacrificing excellent statistical performance. We also consider the Lagged Fibonacci Generator (LFib), a simple but powerful recurrence based PRNG. Additionally, we look at using CUDA for massively parallel experimentation with a larger number of experiments for each algorithm. CUDA has native implementations of the XORWow, Mersenne Twister, and other parallelizable PRNG algorithms.

        Through this study, we seek to devise a strategy of combining PRNGs and empirical gradient descent to navigate complex search spaces efficiently. This combined approach will improve the optimization algorithm convergence as well as the ability to find the global optima. This combined approach aims utilizes the best parts of both randomness and deterministic refinement in solving high-dimensional optimization problems.

    \section{Methods}

        \subsection{Pseudo-random Number Generation}

            \subsubsection{Xoshiro256++}

            \subsubsection{Mersenne Twister}

        \subsection{Benchmarks}

        \subsection{Empirical Gradient Descent}

    \section{Results}

        \begin{table}[h]
        \begin{centering}
            \begin{tabular}{|c||c|c|c|c|c|c|}
                \hline
                            & Average & Standard Deviation & Minimum & Maximum & Median & Time [s] \\
                \hline
                \hline
                Ackley's One & 587.19 & 60.62 & 424.5 & 705.34 & 588.17 & 0.438 \\
                \hline
                Ackley's Two & 652.36 & 14.56 & 602.65 & 677.75 & 653.94 & 0.053 \\
                \hline
                1st De Jong's & 104726.64 & 14731.69 & 79401.66 & 136052.36 & 105329.46 & 0.022 \\
                \hline
                Egg Holder & -67.51 & 1564.88 & -3543.09 & 3168.68 & -199.16 & 0.119 \\
                \hline
                Griewangk & 635.73 & 129.05 & 354.25 & 841.71 & 663.16 & 0.065 \\
                \hline
                Rastrigin & $2.6 \times 10^{6}$ & 511998.81 & $1.6 \times 10^6$ & $2.9 \times 10^6$ & $2.5 \times 10^6$ & 0.025 \\
                \hline
                Rosenbrock & $5.5 \times 10^{10}$ & $1.5 \times 10^{10}$ & $2.6 \times 10^{10}$ & $8.2 \times 10^{10}$ & $5.6 \times 10^{10}$ & 0.055 \\
                \hline
                Schwefel's & 12265.91 & 898.52 & 10669.58 & 15107.91 & 12154.59 & 0.044 \\
                \hline
                SESW & -20.89 & 1.31 & -23.97 & -18.68 & -20.59 & 0.083 \\
                \hline
                SVSW & 94.07 & 7.9 & 77.61 & 122.17 & 93.86 & 0.081 \\
                \hline
            \end{tabular}
            \caption{Julia with Xoshiro256++}
        \end{centering}
        \end{table}

    \section{Discussion}
        Compare empirical gradient to other standard methods in numerical optimization of geometric structures:
        \begin{itemize}
            \item Barzilai-Borwein method
            \item Wolfe conditions and Line search
            \item Conjugate gradient method
            \item Proximal gradient method
            \item Stochastic gradient descent
            \item Mirror descent
            \item Broyden–Fletcher–Goldfarb–Shanno algorithm
            \item Davidon–Fletcher–Powell formula
            \item Nelder–Mead method
            \item Gauss–Newton algorithm
            \item Quantum annealing
            \item Hill climbing
        \end{itemize}

    \section{Conclusion}

\end{document}
